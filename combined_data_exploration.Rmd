---
title: "GLMs for Car Insurance Pricing: Combined Data Exploration"
author: "Mick Cooney <mickcooney@gmail.com>"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    number_sections: true
    fig_caption: yes
    theme: cerulean
  pdf_document: default
---


```{r knit_opts, include=FALSE}
rm(list = ls())

library(tidyverse)
library(forcats)
library(lubridate)
library(scales)
library(GGally)
library(cowplot)
library(Rtsne)
library(feather)


options(width = 90L
       ,warn  = 1)

knitr::opts_chunk$set(tidy       = FALSE
                     ,cache      = FALSE
                     ,warning    = FALSE
                     ,message    = FALSE
                     ,fig.height =     8
                     ,fig.width  =    11
                     )


set.seed(42)

source("custom_functions.R")
```


# Introduction

This workbook performs the basic data exploration of the dataset.

```{r set_exploration_params, echo=TRUE}
dataexp_level_exclusion_threshold <- 100

dataexp_cat_level_count <- 40
dataexp_hist_bins_count <- 50
```


# Load Data

First we load the dataset.

```{r load_dataset, echo=TRUE}
policy_data_tbl <- read_feather('data/policy_data.feather')
claim_data_tbl  <- read_feather('data/claim_data.feather')

claim_agg_tbl <- claim_data_tbl %>%
    group_by(policy_id) %>%
    summarise(claim_count = n()
             ,claim_total = sum(claim_amount)
    )              


rawdata_tbl <- policy_data_tbl %>%
    left_join(claim_agg_tbl, by = 'policy_id') %>%
    as_data_frame

glimpse(rawdata_tbl)
```


## Perform Quick Data Cleaning


```{r perform_simple_datatype_transforms, echo=TRUE}
### _TEMPLATE_
### Do simple datatype transforms and save output in data_tbl
data_tbl <- rawdata_tbl

names(data_tbl) <- data_tbl %>% names %>% clean_names

glimpse(data_tbl)
```

We want to replace missing values on the claim data with zeros, and also need
to check consistency between the claim counts in both the policy and claim
data sets. In the event of a discrepancy, we will go with the claim count
from the claim data.

```{r fix_claim_fields, echo=TRUE}
data_tbl <- data_tbl %>%
    replace_na(list(claim_count = 0, claim_total = 0))

data_tbl %>% filter(claim_count != claim_nb)

data_tbl <- data_tbl %>%
    select(-claim_nb)

data_tbl %>% glimpse
```


```{r, echo=FALSE}
#knitr::knit_exit()
```


## Create Derived Variables

We now create derived features useful for modelling. These values are
new variables calculated from existing variables in the data.

```{r create_derived_variables, echo=TRUE}
data_tbl <- data_tbl

glimpse(data_tbl)
```



```{r, echo=FALSE}
#knitr::knit_exit()
```


## Check Missing Values

Before we do anything with the data, we first check for missing values
in the dataset. In some cases, missing data is coded by a special
character rather than as a blank, so we first correct for this.

```{r replace_missing_character, echo=TRUE}
### _TEMPLATE_
### ADD CODE TO CORRECT FOR DATA ENCODING HERE
```

With missing data properly encoded, we now visualise the missing data in a
number of different ways.

### Univariate Missing Data

We first examine a simple univariate count of all the missing data:

```{r missing_data_univariate_count, echo=TRUE}
row_count <- data_tbl %>% nrow

missing_univariate_tbl <- data_tbl %>%
    summarise_all(funs(sum(is.na(.)))) %>%
    gather('variable','missing_count') %>%
    mutate(missing_prop = missing_count / row_count)

ggplot(missing_univariate_tbl) +
    geom_bar(aes(x = fct_reorder(variable, -missing_prop), weight = missing_prop)) +
    scale_y_continuous(labels = comma) +
    xlab("Variable") +
    ylab("Missing Value Proportion") +
    theme(axis.text.x = element_text(angle = 90))
```

We remove all variables where all of the entries are missing

```{r remove_entirely_missing_vars, echo=TRUE}
remove_vars <- missing_univariate_tbl %>%
    filter(missing_count == row_count) %>%
    .[['variable']]

lessmiss_data_tbl <- data_tbl %>%
    select(-one_of(remove_vars))
```

With these columns removed, we repeat the exercise.

```{r missing_data_univariate_count_redux, echo=TRUE}
missing_univariate_tbl <- lessmiss_data_tbl %>%
    summarise_all(funs(sum(is.na(.)))) %>%
    gather('variable','missing_count') %>%
    mutate(missing_prop = missing_count / row_count)

ggplot(missing_univariate_tbl) +
    geom_bar(aes(x = fct_reorder(variable, -missing_prop), weight = missing_prop)) +
    scale_y_continuous(labels = comma) +
    xlab("Variable") +
    ylab("Missing Value Proportion") +
    theme(axis.text.x = element_text(angle = 90))
```


To reduce the scale of this plot, we look at the top twenty missing data counts.

```{r missing_data_univariate_top10_count, echo=TRUE}
missing_univariate_top_tbl <- missing_univariate_tbl %>%
    arrange(desc(missing_count)) %>%
    top_n(n = 50, wt = missing_count)

ggplot(missing_univariate_top_tbl) +
    geom_bar(aes(x = fct_reorder(variable, -missing_prop), weight = missing_prop)) +
    scale_y_continuous(labels = comma) +
    xlab("Variable") +
    ylab("Missing Value Proportion") +
    theme(axis.text.x = element_text(angle = 90))
```



### Multivariate Missing Data

It is useful to get an idea of what combinations of variables tend to have
variables with missing values simultaneously, so to construct a visualisation
for this we create a count of all the times given combinations of variables
have missing values, producing a heat map for these combination counts.

```{r missing_data_matrix, echo=TRUE}
missing_plot_tbl <- rawdata_tbl %>%
    mutate_all(funs(is.na)) %>%
    mutate_all(funs(as.numeric)) %>%
    mutate(label = do.call(paste0, (.))) %>%
    group_by(label) %>%
    summarise_all(funs(sum)) %>%
    arrange(desc(label)) %>%
    select(-label) %>%
    mutate(rowid = do.call(pmax, (.))) %>%
    gather('col','count', -rowid) %>%
    mutate(Proportion = count / row_count
          ,rowid      = round(rowid / row_count, 4)
    )

ggplot(missing_plot_tbl) +
    geom_tile(aes(x = col, y = as.factor(rowid), fill = Proportion), height = 0.8) +
    scale_fill_continuous(labels = comma) +
    scale_x_discrete(position = 'top') +
    xlab("Variable") +
    ylab("Missing Value Proportion") +
    theme(axis.text.x = element_text(angle = 90))
```

This visualisation takes a little explaining.

Each row represents a combination of variables with simultaneous missing
values. For each row in the graphic, the coloured entries show which particular
variables are missing in that combination. The proportion of rows with that
combination is displayed in both the label for the row and the colouring for
the cells in the row.

## Inspect High-level-count Categorical Variables

With the raw data loaded up we now remove obvious unique or near-unique
variables that are not amenable to basic exploration and plotting.

```{r find_highlevelcount_categorical_variables, echo=TRUE}
coltype_lst <- create_coltype_list(data_tbl)

catvar_valuecount_tbl <- data_tbl %>%
    summarise_at(coltype_lst$split$discrete
                ,function(x) length(unique(x))) %>%
    gather('var_name', 'level_count') %>%
    arrange(-level_count)

print(catvar_valuecount_tbl)

row_count <- nrow(data_tbl)

cat(paste0("Dataset has ", row_count, " rows\n"))
```

Now that we a table of the counts of all the categorical variables we can
automatically exclude unique variables from the exploration, as the level
count will match the row count.

```{r remove_id_variables, echo=TRUE}
unique_vars <- catvar_valuecount_tbl %>%
    filter(level_count == row_count) %>%
    .[["var_name"]]

print(unique_vars)

explore_data_tbl <- data_tbl %>%
    select(-one_of(unique_vars))
```

Having removed the unique identifier variables from the dataset, we
may also wish to exclude categoricals with high level counts also, so
we create a vector of those variable names.

```{r collect_highcount_variables, echo=TRUE}
highcount_vars <- catvar_valuecount_tbl %>%
    filter(level_count >= dataexp_level_exclusion_threshold
          ,level_count < row_count) %>%
    .[["var_name"]]

cat(paste0(highcount_vars, collapse = ', '))
```

We now can continue doing some basic exploration of the data. We may
also choose to remove some extra columns from the dataset.

```{r drop_variables, echo=TRUE}
### You may want to comment out these next few lines to customise which
### categoricals are kept in the exploration.
drop_vars <- c(highcount_vars)

if(length(drop_vars) > 0) {
    explore_data_tbl <- explore_data_tbl %>%
        select(-one_of(drop_vars))

    cat(paste0(drop_vars, collapse = ', '))
}
```

```{r, echo=FALSE}
#knitr::knit_exit()
```


# Write to Disk

```{r write_to_disk, echo=TRUE}
write_feather(data_tbl, path = 'data/policyaggclaim_data.feather')
```




# Univariate Data Exploration

Now that we have loaded the data we can prepare it for some basic data
exploration. We first exclude the variables that are unique
identifiers or similar, and tehen split the remaining variables out
into various categories to help with the systematic data exploration.


```{r separate_exploration_cols, echo=TRUE}
coltype_lst <- create_coltype_list(explore_data_tbl)

print(coltype_lst)
```


## Logical Variables

Logical variables only take two values: TRUE or FALSE. It is useful to see
missing data as well though, so we also plot the count of those.

```{r create_univariate_logical_plots, echo=TRUE, warning=FALSE}
logical_vars <- coltype_lst$split$logical

for(plot_varname in logical_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    na_count <- explore_data_tbl %>% .[[plot_varname]] %>% is.na %>% sum

    explore_plot <- ggplot(explore_data_tbl) +
        geom_bar(aes_(x = plot_varname)) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0('Barplot of Counts for Variable: ', plot_varname
                      ,' (', na_count, ' missing values)')) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    plot(explore_plot)
}
```


## Numeric Variables

Numeric variables are usually continuous in nature, though we also have
integer data.

```{r create_univariate_numeric_plots, echo=TRUE, warning=FALSE}
numeric_vars <- coltype_lst$split$continuous

for(plot_varname in numeric_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    plot_var <- explore_data_tbl %>% .[[plot_varname]]
    na_count <- plot_var %>% is.na %>% sum

    plot_var %>% summary %>% print

    explore_plot <- ggplot(explore_data_tbl) +
        geom_histogram(aes_string(x = plot_varname), bins = dataexp_hist_bins_count) +
        geom_vline(xintercept = mean  (plot_var, na.rm = TRUE), colour = 'red',   size = 1.5) +
        geom_vline(xintercept = median(plot_var, na.rm = TRUE), colour = 'green', size = 1.5) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_x_continuous(labels = comma) +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0('Histogram Plot for Variable: ', plot_varname
                      ,' (', na_count, ' missing values)')
               ,subtitle = '(red line is mean, green line is median)')

    print(explore_plot)
}
```

## Categorical Variables

Categorical variables only have values from a limited, and usually fixed,
number of possible values

```{r create_univariate_categorical_plots, echo=TRUE, warning=FALSE}
categorical_vars <- coltype_lst$split$discrete

for(plot_varname in categorical_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    na_count <- explore_data_tbl %>% .[[plot_varname]] %>% is.na %>% sum

    plot_tbl <- explore_data_tbl %>%
        .[[plot_varname]] %>%
        as.character %>%
        fct_lump(n = dataexp_cat_level_count) %>%
        fct_count

    explore_plot <- ggplot(plot_tbl) +
        geom_bar(aes(x = fct_reorder(f, -n), weight = n)) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0('Barplot of Counts for Variable: ', plot_varname
                      ,' (', na_count, ' missing values)')) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    plot(explore_plot)
}
```


## Date/Time Variables

Date/Time variables represent calendar or time-based data should as time of the
day, a date, or a timestamp.

```{r create_univariate_datetime_plots, echo=TRUE, warning=FALSE}
datetime_vars <- coltype_lst$split$datetime

for(plot_varname in datetime_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    plot_var <- explore_data_tbl %>% .[[plot_varname]]
    na_count <- plot_var %>% is.na %>% sum

    plot_var %>% summary %>% print

    explore_plot <- ggplot(explore_data_tbl) +
        geom_histogram(aes_string(x = plot_varname), bins = dataexp_hist_bins_count) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0('Barplot of Dates/Times in Variable: ', plot_varname
                      ,' (', na_count, ' missing values)'))

    plot(explore_plot)
}
```


```{r, echo=FALSE}
#knitr::knit_exit()
```


# Bivariate Data Exploration

We now move on to looking at bivariate plots of the data set.

## Pairs Plots

Pairs plots area very useful way of getting a quick idea of the relationships
between variables in a data set.

Unfortunately, they do not scale well. Too many rows (say more than 5,000) can
slow down the rendering, and more than 10 columns can make the plots
uninterpretable as each cell is too small.

The technique is useful, so to circumvent these issues we sample the dataset.
We select random columns and rows, and make a pairs plot of the subset,
repeating this process for a number of iterations.

```{r plot_pairsplot, echo=TRUE, warning=FALSE, message=FALSE, fig.width = 20, fig.height=15}
dataexp_pairsplot_itercount <-     3
dataexp_pairsplot_colcount  <-     5
dataexp_pairsplot_rowcount  <-  5000

if(ncol(data_tbl) > dataexp_pairsplot_colcount ||
   nrow(data_tbl) > dataexp_pairsplot_rowcount) {

    ### Ugly hack to work around current dplyr bug for mutate_if
    if(any(sapply(explore_data_tbl, is.logical))) {
        conv_tbl <- explore_data_tbl %>%
            mutate_if(is.logical, as.factor)
    } else {
        conv_tbl <- explore_data_tbl
    }

    conv_tbl <- conv_tbl %>%
        mutate_if(function(x) (is.character(x) || is.factor(x)) && !all(is.na(x))
                 ,function(x) fct_lump(x, n = 9))


    for(i in 1:dataexp_pairsplot_itercount) {
        cat("--\n")
        cat(paste0("Pairs plot iter: ", i, "\n"))

        pairs_tbl <- conv_tbl %>%
            create_ggpairs_tbl(sample_cols = dataexp_pairsplot_colcount
                              ,sample_rows = dataexp_pairsplot_rowcount
                               )

        cat(paste0("Columns: ", paste0(names(pairs_tbl), collapse = ', '), "\n"))

        pairs_tbl %>%
            ggpairs(cardinality_threshold = NULL
                   ,lower = list(combo = wrap('facethist', bins = 25))
                   ) %>%
            print
    }
} else {
    ggpairs(data_tbl) %>% print
}
```

```{r free_memory_bivariateplots, echo=FALSE, warning=FALSE}
rm(conv_tbl, pairs_tbl)
```


## Facet Plots on Variables

We want to look at how the variables split on the logical variables as this is
a very natural way to observe the data.

```{r bivariate_facet_data, echo=TRUE}
### _TEMPLATE_
facet_varname <- 'gas'


facet_count_max <- 3
facet_formula   <- formula(paste0("~ as.factor(", facet_varname, ")"))
```


### Logical Variables

For logical variables we facet on barplots of the levels, comparing TRUE,
FALSE and missing data.

```{r create_bivariate_logical_plots, echo=TRUE}
logical_vars <- logical_vars[!logical_vars %in% facet_varname]

for(plot_varname in logical_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    filter_formula <- formula(paste0("~ !is.na(", plot_varname, ")"))

    plot_tbl <- data_tbl %>% filter_(filter_formula)

    facet_count <- plot_tbl %>%
        .[[facet_varname]] %>%
        unique %>%
        length %>%
        min(facet_count_max)

    explore_plot <- ggplot(plot_tbl) +
        geom_bar(aes_string(x = plot_varname)) +
        facet_wrap(facet_formula, scales = 'free') +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0(facet_varname, '-Faceted Barplots for Variable: ', plot_varname)) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    plot(explore_plot)
}
```


### Numeric Variables

For numeric variables, we facet on histograms of the data.

```{r create_bivariate_numeric_plots, echo=TRUE}
for(plot_varname in numeric_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    filter_formula <- formula(paste0("~ !is.na(", plot_varname, ")"))

    plot_tbl <- data_tbl %>% filter_(filter_formula)

    facet_count <- plot_tbl %>%
        .[[facet_varname]] %>%
        unique %>%
        length %>%
        min(facet_count_max)

    explore_plot <- ggplot(plot_tbl) +
        geom_histogram(aes_string(x = plot_varname), bins = dataexp_hist_bins_count) +
        facet_wrap(facet_formula, scales = 'free') +
        xlab(plot_varname) +
        ylab("Count") +
        scale_x_continuous(labels = comma) +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0(facet_varname, '-Faceted Histogram for Variable: ', plot_varname)) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    print(explore_plot)
}
```

### Categorical Variables

We treat categorical variables like logical variables, faceting the barplots
of the different levels of the data.

```{r create_bivariate_categorical_plots, echo=TRUE}
categorical_vars <- categorical_vars[!categorical_vars %in% facet_varname]

for(plot_varname in categorical_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    filter_formula <- formula(paste0("~ !is.na(", plot_varname, ")"))

    plot_tbl <- data_tbl %>% filter_(filter_formula)

    facet_count <- plot_tbl %>%
        .[[facet_varname]] %>%
        unique %>%
        length %>%
        min(facet_count_max)

    explore_plot <- ggplot(plot_tbl) +
        geom_bar(aes_string(x = plot_varname)) +
        facet_wrap(facet_formula, scales = 'free') +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0(facet_varname, '-Faceted Histogram for Variable: ', plot_varname)) +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    plot(explore_plot)
}
```


### Date/Time Variables

Like the univariate plots, we facet on histograms of the years in the dates.

```{r create_bivariate_datetime_plots, echo=TRUE}
for(plot_varname in datetime_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    filter_formula <- formula(paste0("~ !is.na(", plot_varname, ")"))

    plot_tbl <- data_tbl %>%
        filter_(filter_formula) %>%
        mutate_(plot_vartmp = plot_varname) %>%
        mutate(plot_var = year(plot_vartmp))

    facet_count <- plot_tbl %>%
        .[[facet_varname]] %>%
        unique %>%
        length %>%
        min(facet_count_max)

    explore_plot <- ggplot(plot_tbl) +
        geom_bar(aes(x = plot_var)) +
        facet_wrap(facet_formula, scales = 'free') +
        xlab(plot_varname) +
        ylab("Count") +
        scale_y_continuous(labels = comma) +
        ggtitle(paste0(facet_varname, '-Faceted Histogram for Variable: ', plot_varname))

    plot(explore_plot)
}
```

```{r free_memory_facetplot, echo=FALSE}
rm(plot_var, plot_tbl)
```

## Other Bivariate Plots

From looking at the various facetting and pairs plots we now look at some
individual bivariate plots to see if we can learn anything useful from them.

### density vs region

We would expect the different regions to have different population density
distributions, so we construct a boxplot of these densities by region to see
how this looks

```{r bivariate_boxplot_density_region, echo=TRUE}
ggplot(data_tbl) +
    geom_boxplot(aes(x = region, y = density)) +
    scale_y_continuous(labels = comma) +
    xlab("Region") +
    ylab("Density") +
    ggtitle("Boxplot of Population Density by Region")
```

We will also look at a histogram of these density values, but facet by region.

```{r bivariate_facetplot_density_region, echo=TRUE, fig.height=16}
ggplot(data_tbl) +
    geom_histogram(aes(x = density), bins = 50) +
    facet_wrap(~region, scales = 'free_y', ncol = 2) +
    scale_y_continuous(labels = comma) +
    xlab("Density") +
    ylab("Frequency Count") +
    ggtitle("density Histogram by region")
```


### exposure by region

In some of the later plots we will be checking some geographical distributions
of claim counts. Aware that the exposure distributions may not be the same, we
first want to see how these compare.

```{r plot_exposure_by_policycount, echo=TRUE}
exposure_tbl <- data_tbl %>%
    group_by(region) %>%
    summarise(total_exposure = sum(exposure)
             ,policy_count   = n())

ggplot(exposure_tbl) +
    geom_point(aes(x = policy_count, y = total_exposure, colour = region)) +
    geom_text (aes(x = policy_count, y = total_exposure, colour = region, label = region)) +
    scale_x_continuous(labels = comma) +
    scale_y_continuous(labels = comma) +
    xlab("Policy Count") +
    ylab("Total Exposure (Policy Years") +
    ggtitle("Plot of Policy Counts and Total Exposures per Region")

```



### claim_count by region

We want to see if there is much of a geographical influence on claim counts
in general, so we plot these separately also.

```{r bivariate_facetplot_claimcount_region, echo=TRUE, fig.height=16}
ggplot(data_tbl) +
    geom_bar(aes(x = claim_count)) +
    facet_wrap(~region, scales = 'free', ncol = 2) +
    expand_limits(x = 4) +
    scale_y_continuous(labels = comma) +
    xlab("Claim Count") +
    ylab("Frequency Count") +
    ggtitle("claim_count Barplot by region")
```

We want to standardise these counts as not all regions will be equal, so we
will look at this as a proportion of total policies and total exposures. The
previous exploration suggests this will have little effect, but it is better
to be sure as it is very quick to do.

```{r policy_region_count, echo=TRUE, fig.height=16}
policy_region_tbl <- data_tbl %>%
    count(region)

policyprop_tbl <- data_tbl %>%
    left_join(policy_region_tbl, by = 'region') %>%
    group_by(region, claim_count) %>%
    summarise(count = n()
             ,prop  = count / max(n)
              )


ggplot(policyprop_tbl) +
    geom_col(aes(x = claim_count, y = prop)) +
    facet_wrap(~region, ncol = 2) +
    coord_flip() +
    xlab("Proportion of Policies") +
    ylab("Claims on Policy") +
    ggtitle("Claim Proportion Barplot by Region")
```

We filter out the policies with no claims and re-plot.

```{r policy_region_count_claimsonly, echo=TRUE, fig.height=16}
ggplot(policyprop_tbl %>% filter(claim_count > 0)) +
    geom_col(aes(x = claim_count, y = prop)) +
    facet_wrap(~region, ncol = 2) +
    coord_flip() +
    xlab("Proportion of Policies") +
    ylab("Claims on Policy") +
    ggtitle("Claim Proportion Barplot by Region")
```


Looking at this a different way, we will produce the barplots by region, and
facet by claim_count.


```{r bivariate_barplot_claim_proportion_region_count, echo=TRUE}
ggplot(policyprop_tbl %>% filter(claim_count > 0)) +
    geom_col(aes(x = region, y = prop)) +
    facet_wrap(~claim_count, ncol = 2, scales = 'free_y') +
    xlab("Region") +
    ylab("Proportion of Policies") +
    ggtitle("Claim Proportion Barplot by Count of Claims and Region")
```


### region by car_age

While unlikely, it is possible there is a geospatial distribution of car ages,
so we construct a plot to look at that.

```{r bivariate_boxplot_region_carage, echo=TRUE}
ggplot(data_tbl) +
    geom_boxplot(aes(x = region, y = car_age)) +
    xlab("Region") +
    ylab("Age of Car (years)") +
    ggtitle("Boxplot of car_age by region")
```

A number of these cars are exceptionally old and are likely vintage. We may
need to model these cars separately.


### claim_total by region

We want to take a look at total claims paid by region to see if there is a
geospatial effect for claims paid out.

```{r bivariate_boxplot_claimtotal_region, echo=TRUE}
ggplot(data_tbl %>% filter(claim_total > 0)) +
    geom_boxplot(aes(x = region, y = claim_total)) +
    scale_y_log10(labels = scales::dollar) +
    xlab("Region") +
    ylab("Total Claims Paid on Policy") +
    ggtitle("Boxplot of Total Claims on a Policy by region")
```

If we filter out the large claims (those above 25,000), we can look at large
losses only.

```{r bivariate_boxplot_claimtotal_region_attritional, echo=TRUE}
ggplot(data_tbl %>% filter(claim_total <= 25000)) +
    geom_boxplot(aes(x = region, y = claim_total)) +
    scale_y_log10(labels = scales::dollar) +
    xlab("Region") +
    ylab("Total Claims Paid on Policy") +
    ggtitle("Boxplot of Total Attritional Losses on a Policy by region")
```


```{r, echo=FALSE}
knitr::knit_exit()
```




# Multivariate Visualisation

Having looked at the pairs plots we also look at multivariate plots of all
the data. We do this using techniques known as 'multidimensional scaling' or
MDS.

Many of these techniques do not scale well beyond a few thousand data points,
so we repeat our sampling trick as before and create multiple plots from these
samples.


```{r redefine_numeric_vars, echo=TRUE}
numeric_vars <- create_coltype_list(explore_data_tbl)$split$continuous
```


## Multidimensional Scaling

We start with classic multidimensional scaling, also called 'principal
coordinates analysis', which is done in R via the function `cmdscale`.

```{r create_mds_plots, echo=TRUE}
mds_iter_count   <-    4
mds_sample_count <- 2000

row_ids <- data_tbl %>%
    select(one_of(numeric_vars)) %>%
    complete.cases


### _TEMPLATE_
### Choosing the first variable in the categorical list by default. You probably
### want to change that.
colour_var <- 'power'


input_tbl <- data_tbl %>%
    select(one_of(c(numeric_vars, colour_var))) %>%
    filter(row_ids)

construct_mds_plot <- function(mds_tbl) {
    num_mds_dist <- mds_tbl %>% select(one_of(numeric_vars)) %>% dist

    num_mds <- cmdscale(num_mds_dist, k = 2, eig = TRUE)

    mds_tbl <- mds_tbl %>%
        mutate(mds_d1 = num_mds$points[,1]
              ,mds_d2 = num_mds$points[,2])

    mds_plot <- ggplot(mds_tbl) +
        geom_point(aes_string(x = 'mds_d1', y = 'mds_d2', colour = colour_var)) +
        xlab("MDS Dim 1") +
        ylab("MDS Dim 2")

    return(mds_plot)
}


mds_lst <- create_sampled_output(input_tbl, construct_mds_plot, mds_sample_count, mds_iter_count)


for(i in 1:length(mds_lst)) {
    cat("--\n")
    cat(paste0("MDS plot iter: ", i, "\n"))

    mds_lst[[i]] %>% print
}
```

```{r, echo=FALSE}
#knitr::knit_exit()
```

## t-SNE Plots

One standard method for doing this is t-SNE, t-distributed Stochastic
Neighbourhood Embedding. This algorithm is a type of dimensionality reduction
- it constructs a lower-dimensional set of data from the original dataset by
attempting the minimise the Kullback-Lieber divergence between the original
and target datasets.

t-SNE requires unique datapoints, so to ensure we do not pass repeated rows
at any point, we may add a small amount of noise to the numeric columns to
ensure uniqueness - t-SNE is a probabilistic process so this should not affect
our output very much.

As with previous methods, we take samples from larger datasets and plot outputs
from multiple samples.

```{r create_tsne_plots, echo=TRUE}
tsne_iter_count   <-     4
tsne_sample_count <-  5000

row_ids <- data_tbl %>%
    select(one_of(numeric_vars)) %>%
    complete.cases


### _TEMPLATE_
### Choosing the first variable in the categorical list by default. You probably
### want to change that.
colour_var <- 'power'


input_tbl <- data_tbl %>%
    select(one_of(c(numeric_vars, colour_var))) %>%
    jitter_numeric_variables %>%
    filter(row_ids)


construct_tsne_plot <- function(tsne_tbl) {
    data_tsne <- Rtsne(tsne_tbl %>% select(one_of(numeric_vars)), theta = 0.9)

    tsne_tbl$tsne_d1 <- data_tsne$Y[,1]
    tsne_tbl$tsne_d2 <- data_tsne$Y[,2]

    tsne_plot <- ggplot(tsne_tbl) +
        geom_point(aes_string(x = 'tsne_d1', y = 'tsne_d2', colour = colour_var)
                  ,size = 0.5) +
        xlab("t-SNE Dim 1") +
        ylab("t-SNE Dim 2")

    return(tsne_plot)
}


tsne_lst <- create_sampled_output(input_tbl, construct_tsne_plot, tsne_sample_count, tsne_iter_count)


for(i in 1:length(tsne_lst)) {
    cat("--\n")
    cat(paste0("t-SNE plot iter: ", i, "\n"))

    tsne_lst[[i]] %>% print
}
```


```{r, echo=FALSE}
#knitr::knit_exit()
```


# Outlier Identification

Another important part of data exploration is the identification of possible
outliers, and we approach this in multiple ways.

In keeping with the methodical approach we start with a univariate
perspective, looking at each numerical variable by itself and plotting the
values in the variable both with and without the outliers.

## Univariate Outlier Plots

```{r outlier_univariate_numeric_plots, echo=TRUE, warning=FALSE, fig.width = 20, fig.height=15}
for(plot_varname in numeric_vars) {
    cat("--\n")
    cat(paste0(plot_varname, '\n'))

    plot_var <- data_tbl %>% .[[plot_varname]]

    outlier_point <- identify_univariate_outliers(plot_var)

    no_outlier_vals <- plot_var[outlier_point]

    all_plot <- ggplot() +
        geom_histogram(aes(x = plot_var), bins = dataexp_hist_bins_count) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_x_continuous(labels = comma) +
        scale_y_continuous(labels = comma) +
        ggtitle("All Data") +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    no_outlier_plot <- ggplot() +
        geom_histogram(aes(x = no_outlier_vals), bins = dataexp_hist_bins_count) +
        xlab(plot_varname) +
        ylab("Count") +
        scale_x_continuous(labels = comma) +
        scale_y_continuous(labels = comma) +
        ggtitle("No Outliers") +
        theme(axis.text.x = element_text(angle = 90, vjust = 0.5))

    plot_grid(all_plot, no_outlier_plot, ncol = 2) %>% print
}
```

We use the above plots to decide if we need to remove certain extreme values
from the dataset.

```{r remove_univariate_outlier, echo=TRUE}
# We place basic logic for identifying univariate outliers here
#data_filt_tbl <- data_tbl %>% mutate(uni_outlier = ifelse(val > 100))
```


```{r output_data, echo=TRUE}

```

```{r, echo=FALSE}
knitr::knit_exit()
```


## Multivariate Outliers

Sometimes outliers are so not because of individual values, but because the
particular combination of values is unusual and so may have undue influence
on the analysis.

Our univariate approach will not discover these outliers, we need to consider
multiple dimensions at once. In analogy to the univariate analysis, there are
multiple possible approaches we might take, but we start with a standard and
common approach: we calculate the Mahalanobis distance for each data point
and then look to label any data points with very high distances as being
'outliers'.

The Mahalanobis distance, $D_M$, can be thought of as the multivariate
equivalent of 'number of standard deviations away from the mean'. If we
consider the data to be a set of multivariate values with mean vector
$\mathbf{\mu}$ and covariance $\Sigma$, the Mahalanobis distance for any
individual datapoint $\mathbf{x}$ is calculated as follows:

$$
D_M(\mathbf{x}) = \sqrt{(\mathbf{x} - \mathbf{\mu})^T \, \Sigma^{-1} \, (\mathbf{x} - \mathbf{\mu})}
$$

We think of the Mahalanobis distance as being an extension of the standard
Euclidean distance where the correlation amongst variables is accounted for.
Thus, data points that differ mainly along higher-correlated axes are
considered 'closer' than datapoints different along less-correlated axes.

To perform this calculation we need estimates for $\mu$ and $\Sigma$. As we
are assuming the presence of outliers, we use robust methods to obtain our
estimates of the mean and the covariance. The `cov.rob()` function from the
`MASS` package is used. For simplicity, we start with just the numerical
variables, extending this approach after.

```{r use_numeric_variables, echo=TRUE}
num_data_tbl <- data_tbl %>%
    select(one_of(numeric_vars))

complete_flag <- num_data_tbl %>% complete.cases
```

Now that we have a dataset of numeric variables with no missing data we
calculate robust estimates for the mean and covariance of the data.

```{r estimate_data_mean_covariance, echo=TRUE}
mcd_estimate <- num_data_tbl %>%
    filter(complete_flag) %>%
    robustbase::covMcd()
```

With robust estimates for both the mean and covariance, we now calculate the
Mahalanobis distance for each of the datapoints.

```{r numeric_vars_mahalanobis_dist, echo=TRUE}
m_dist <- rep(NA, row_count)
m_dist[complete_flag] <- num_data_tbl %>%
    filter(complete_flag) %>%
    mahalanobis(center = mcd_estimate$center, cov = mcd_estimate$cov) %>%
    sqrt

data_tbl <- data_tbl %>%
    mutate(m_dist = m_dist)
```

We have calculated the Mahalanobis distance and appended it to the data, so
we look at a cumulative plot of these distances to see if any are so far
removed from the data we may consider labelling them as outliers.

```{r identify_mahalanobis_outliers, echo=TRUE}
cutoff_percentile <- 0.99

ggplot(data_tbl %>% filter(!is.na(m_dist))) +
    geom_line(aes(x = seq_along(m_dist) / length(m_dist), y = sort(m_dist))) +
    geom_vline(aes(xintercept = cutoff_percentile), colour = 'red') +
    xlab("Quantile Percentage") +
    ylab("Mahalanobis Distance") +
    ggtitle("Percentile Plot of Mahalanobis Distance")
```

```{r label_datapoints_outliers, echo=TRUE}
cutoff_distance <- quantile(m_dist, probs = cutoff_percentile, na.rm = TRUE)

data_tbl <- data_tbl %>%
    mutate(mcd_outlier = m_dist >= cutoff_distance)
```

Having labelled data points as outliers, we now redo this percentile plot to
see how it looks

```{r outlier_removed_mahalanobis_percentile_plot, echo=TRUE}
ggplot(data_tbl %>% filter(mcd_outlier == FALSE)) +
    geom_line(aes(x = seq_along(m_dist) / length(m_dist), y = sort(m_dist))) +
    geom_vline(aes(xintercept = cutoff_percentile), colour = 'red') +
    xlab("Quantile Percentage") +
    ylab("Mahalanobis Distance") +
    ggtitle("No-Outlier Percentile Plot of Mahalanobis Distance")
```
